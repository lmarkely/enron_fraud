{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points: 145\n",
      "Total number of features: 17\n",
      "PCA explained_variance_ratio_ [ 0.77628412  0.07772042]\n",
      "LDA explained variance: [ 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Raga/anaconda/envs/enron/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "### Include all quantitative features. In addition, 'std_from_poi' and\n",
    "### 'std_to_poi' are standardized feature (see details below).\n",
    "features_list = ['poi','salary', 'deferral_payments', 'total_payments',\n",
    "                 'loan_advances', 'bonus', 'restricted_stock_deferred',\n",
    "                 'deferred_income', 'total_stock_value', 'expenses',\n",
    "                 'exercised_stock_options', 'other', 'long_term_incentive',\n",
    "                 'restricted_stock', 'director_fees','shared_receipt_with_poi',\n",
    "                 'std_from_poi','std_to_poi']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "# Add new features: std_from_poi and std_to_poi by dividing the message\n",
    "# to/from poi by the total sent or received messages, respectively.\n",
    "for key in data_dict:\n",
    "    if (type(data_dict[key]['from_poi_to_this_person']) == int and\n",
    "        type(data_dict[key]['from_messages']) == int):\n",
    "        data_dict[key]['std_from_poi'] = \\\n",
    "        (data_dict[key]['from_poi_to_this_person']/\n",
    "         data_dict[key]['from_messages'])\n",
    "    else:\n",
    "        data_dict[key]['std_from_poi'] = 0\n",
    "    if (type(data_dict[key]['from_this_person_to_poi']) == int and\n",
    "        type(data_dict[key]['to_messages']) == int):\n",
    "        data_dict[key]['std_to_poi'] = \\\n",
    "        (data_dict[key]['from_this_person_to_poi']/\n",
    "         data_dict[key]['to_messages'])\n",
    "    else:\n",
    "        data_dict[key]['std_to_poi'] = 0\n",
    "my_dataset = data_dict\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "# The followings are the major steps in the analysis:\n",
    "# A. Visualize the data using dimensionality reduction PCA and LDA to gain\n",
    "#    further insight into the data\n",
    "# B. Algorithm selection using repeated nested cross validation to choose\n",
    "#    the algorithm that has highest accuracy\n",
    "# C. Model selection using repeated cross validation to identify the best\n",
    "#    hyperparameter values\n",
    "\n",
    "# The following classification algorithms are used:\n",
    "# 1. Logistic Regression\n",
    "# 2. Random Forest Classifier\n",
    "# 3. KNN Classifier\n",
    "# 4. Support Vector Classifier\n",
    "# 5. Neural Network: Multi-layer Perceptron Classifier\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# For simplicity, rename features as X and labels as y\n",
    "X = features\n",
    "y = labels\n",
    "### First, explore the dataset.\n",
    "### Identify the total number of data points.\n",
    "print 'Total number of data points:',np.shape(X)[0]\n",
    "print 'Total number of features:', np.shape(X)[1]\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit_transform(X_std,y)\n",
    "print 'PCA explained_variance_ratio_', pca.explained_variance_ratio_\n",
    "print 'LDA explained variance:', lda.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Scatterplot Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame(X_std)\n",
    "pg = sns.PairGrid(df)\n",
    "pg.map_diag(plt.hist)\n",
    "pg.map_offdiag(plt.scatter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:enron]",
   "language": "python",
   "name": "conda-env-enron-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
